services:
  # N8N Workflow Automation
  n8n:
    image: n8nio/n8n:stable
    container_name: n8n
    # user: "501:20"  # Commented out to fix permission issues
    ports:
      - "5678:5678"
    environment:
      - N8N_HOST=n8n.dhabas.org
      - N8N_LISTEN_ADDRESS=0.0.0.0
      - N8N_PORT=5678
      - N8N_PROTOCOL=http
      - N8N_EDITOR_BASE_URL=https://n8n.dhabas.org
      - WEBHOOK_URL=https://n8n.dhabas.org/
      - N8N_BASIC_AUTH_USER=${N8N_BASIC_AUTH_USER:-admin}
      - N8N_BASIC_AUTH_PASSWORD=${N8N_BASIC_AUTH_PASSWORD:-admin123}
      - DB_TYPE=${DB_TYPE:-sqlite}
      - EXECUTIONS_PROCESS=${EXECUTIONS_PROCESS:-main}
      - EXECUTIONS_MODE=${EXECUTIONS_MODE:-regular}
      - GENERIC_TIMEZONE=${GENERIC_TIMEZONE:-Asia/Jerusalem}
      - TZ=${TZ:-Asia/Jerusalem}
      - CLAUDE_API_KEY=${CLAUDE_API_KEY}
      # N8N Credential IDs and User Settings
      - GMAIL_CREDENTIAL_ID=${GMAIL_CREDENTIAL_ID}
      - JIRA_CREDENTIAL_ID=${JIRA_CREDENTIAL_ID}
      - TRELLO_CREDENTIAL_ID=${TRELLO_CREDENTIAL_ID}
      - JIRA_USER_EMAIL=${JIRA_USER_EMAIL}
      - VIP_EMAIL_1=${VIP_EMAIL_1}
      - VIP_EMAIL_2=${VIP_EMAIL_2}
      # Prometheus Metrics Configuration (Enhanced)
      - N8N_METRICS=true
      - N8N_METRICS_INCLUDE_QUEUE_METRICS=true
      - N8N_METRICS_INCLUDE_WORKFLOW_ID_LABEL=true
      - N8N_METRICS_INCLUDE_NODE_TYPE_LABEL=true
      - N8N_METRICS_INCLUDE_CREDENTIAL_TYPE_LABEL=true
      - N8N_METRICS_INCLUDE_API_ENDPOINTS=true
      - N8N_METRICS_PREFIX=n8n_
      # Logging Configuration
      - N8N_LOG_LEVEL=info
      - N8N_LOG_OUTPUT=console,file
      - N8N_LOG_FILE_LOCATION=/home/node/.n8n/logs/
      - N8N_LOG_FILE_COUNT_MAX=10
      - N8N_LOG_FILE_SIZE_MAX=16777216
      # Performance and Reliability
      - EXECUTIONS_DATA_PRUNE=true
      - EXECUTIONS_DATA_MAX_AGE=7
      - EXECUTIONS_DATA_PRUNE_MAX_COUNT=10000
      - N8N_DISABLE_UI=false
      - N8N_ENCRYPTION_KEY=${N8N_ENCRYPTION_KEY:-n8n-local-encryption-key-change-this}
      # Workflow execution settings
      - WORKFLOWS_DEFAULT_NAME=My Workflow
      - N8N_DEFAULT_LOCALE=en
      # Security settings (Cloudflare tunnel compatible)
      - N8N_SECURE_COOKIE=false
      - N8N_BLOCK_ENV_ACCESS_IN_NODE=false
      # Cloudflare tunnel optimization
      - N8N_EXTERNAL_FRONTEND_HOOKS_URLS=https://n8n.dhabas.org/
      # Local AI Services
      - OLLAMA_BASE_URL=http://ollama:11434
      - QDRANT_URL=http://qdrant:6333
      - AI_MEMORY_ENABLED=true
      # Fix home directory issue
      - N8N_USER_FOLDER=/home/node/.n8n
    volumes:
      - ./n8n:/home/node/.n8n
      - ./workflows:/home/node/workflows
      - ./data:/data
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "1.0"
        reservations:
          memory: 256M
          cpus: "0.5"
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--quiet",
          "--tries=1",
          "--spider",
          "http://localhost:5678/healthz",
        ]
      interval: 30s
      timeout: 10s
      retries: 3

  # Redis Cache
  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    command: redis-server --requirepass ${REDIS_PASSWORD:-redis123} --maxmemory 100mb --maxmemory-policy allkeys-lru
    volumes:
      - ./data/redis:/data
    deploy:
      resources:
        limits:
          memory: 128M
          cpus: "0.5"
        reservations:
          memory: 64M
          cpus: "0.25"
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 30s
      timeout: 3s
      retries: 3

  # Redis Exporter for Prometheus (Internal monitoring)
  redis-exporter:
    image: oliver006/redis_exporter:latest
    container_name: redis-exporter
    ports:
      - "9121:9121"
    environment:
      - REDIS_ADDR=redis://redis:6379
      - REDIS_PASSWORD=${REDIS_PASSWORD:-redis123}
    deploy:
      resources:
        limits:
          memory: 32M
          cpus: "0.1"
    restart: unless-stopped
    depends_on:
      - redis

  # Nginx Reverse Proxy
  nginx:
    image: nginx:alpine
    container_name: nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/html:/usr/share/nginx/html:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - ./nginx/logs:/var/log/nginx
    deploy:
      resources:
        limits:
          memory: 64M
          cpus: "0.25"
        reservations:
          memory: 32M
          cpus: "0.1"
    depends_on:
      - n8n
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "wget",
          "--quiet",
          "--tries=1",
          "--spider",
          "http://localhost/health",
        ]
      interval: 30s
      timeout: 10s
      retries: 3

  # Nginx Prometheus Exporter (Internal monitoring)
  nginx-exporter:
    image: nginx/nginx-prometheus-exporter:latest
    container_name: nginx-exporter
    ports:
      - "9113:9113"
    command:
      - -nginx.scrape-uri=http://nginx/nginx_status
    deploy:
      resources:
        limits:
          memory: 32M
          cpus: "0.1"
    restart: unless-stopped
    depends_on:
      - nginx

  # Container Monitoring
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: cadvisor
    ports:
      - "8080:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    devices:
      - /dev/kmsg
    privileged: true
    deploy:
      resources:
        limits:
          memory: 200M
          cpus: "0.3"
        reservations:
          memory: 100M
          cpus: "0.1"
    restart: unless-stopped
    command:
      - "--housekeeping_interval=30s"
      - "--max_housekeeping_interval=35s"
      - "--allow_dynamic_housekeeping=true"
      - "--global_housekeeping_interval=30s"
      - "--disable_metrics=disk,diskIO,hugetlb,memory_numa,tcp,udp,percpu,sched,process,referenced_memory"

  # Metrics Collection
  prometheus:
    image: prom/prometheus:v2.45.0
    container_name: prometheus
    ports:
      - "9090:9090"
    environment:
      - PROMETHEUS_N8N_USER=${PROMETHEUS_N8N_USER:-${N8N_BASIC_AUTH_USER:-admin}}
      - PROMETHEUS_N8N_PASSWORD=${PROMETHEUS_N8N_PASSWORD:-${N8N_BASIC_AUTH_PASSWORD:-admin123}}
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./data/prometheus:/prometheus
      - ./scripts:/scripts:ro
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=7d"
      - "--storage.tsdb.retention.size=1GB"
      - "--web.console.libraries=/etc/prometheus/console_libraries"
      - "--web.console.templates=/etc/prometheus/consoles"
      - "--web.enable-lifecycle"
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.5"
        reservations:
          memory: 128M
          cpus: "0.25"
    restart: unless-stopped

  # Metrics Dashboard
  grafana:
    image: grafana/grafana:10.0.0
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-admin123}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource
      # Configuration for subdomain access via Cloudflare tunnel
      - GF_SERVER_ROOT_URL=https://grafana.dhabas.org/
      - GF_SERVER_SERVE_FROM_SUB_PATH=false
      - GF_SERVER_DOMAIN=grafana.dhabas.org
      # Security settings
      - GF_SECURITY_COOKIE_SECURE=false
      - GF_SECURITY_COOKIE_SAMESITE=lax
      # Analytics and telemetry
      - GF_ANALYTICS_REPORTING_ENABLED=false
      - GF_ANALYTICS_CHECK_FOR_UPDATES=false
    volumes:
      - ./data/grafana:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.5"
        reservations:
          memory: 128M
          cpus: "0.25"
    restart: unless-stopped
    depends_on:
      - prometheus

  # Local AI Models (On-Demand Optimized)
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    profiles: ["ai", "always"]
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_MAX_QUEUE=10
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_FLASH_ATTENTION=1
      - OLLAMA_LLM_LIBRARY=cpu
      - OLLAMA_ORIGINS=http://localhost:*,http://n8n:*
      # Performance optimizations
      - OLLAMA_MAX_VRAM=3G
      - OLLAMA_GPU_OVERHEAD=0.1
      - OLLAMA_NOPRUNE=false
      # Metrics and monitoring
      - OLLAMA_METRICS=true
      - OLLAMA_DEBUG=false
    volumes:
      - ./data/ollama:/root/.ollama
      - ./ollama/modelfiles:/root/modelfiles
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: "2.0"
        reservations:
          memory: 1G
          cpus: "1.0"
    restart: "no"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      - redis

  # Optimized model initialization with performance-focused models
  init-ollama:
    image: ollama/ollama:latest
    container_name: ollama-init-models
    profiles: ["ai", "setup"]
    volumes:
      - ./data/ollama:/root/.ollama
      - ./ollama/modelfiles:/root/modelfiles
    entrypoint: /bin/sh
    environment:
      - OLLAMA_HOST=ollama:11434
    command:
      - "-c"
      - |
        echo "Waiting for Ollama service..."
        sleep 10
        echo "Pulling optimized models for digest generation..."
        ollama pull phi3:mini || true
        ollama pull llama3.2:1b || true
        ollama pull qwen2.5:7b || true
        echo "Pulling specialized models for analytical tasks..."
        ollama pull llama3.2:3b || true
        echo "Creating optimized modelfiles..."
        if [ -f /root/modelfiles/digest-phi3 ]; then
          ollama create digest-phi3 -f /root/modelfiles/digest-phi3 || true
        fi
        echo "Model initialization complete"
    depends_on:
      - ollama

  # Vector Database (Lightweight)
  qdrant:
    image: qdrant/qdrant:v1.7.3
    container_name: qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__LOG_LEVEL=INFO
    volumes:
      - ./data/qdrant:/qdrant/storage
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "0.5"
        reservations:
          memory: 256M
          cpus: "0.25"
    restart: unless-stopped

  # SQLite Database for AI Agent Memory
  sqlite-ai:
    image: nouchka/sqlite3:latest
    container_name: sqlite-ai
    profiles: ["ai", "always"]
    volumes:
      - ./data/sqlite:/root/db
      - ./scripts/init-ai-db.sql:/docker-entrypoint-initdb.d/init.sql:ro
    command: tail -f /dev/null
    deploy:
      resources:
        limits:
          memory: 64M
          cpus: "0.2"
        reservations:
          memory: 32M
          cpus: "0.1"
    restart: "no"

  # Node Exporter for system metrics
  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - "--path.procfs=/host/proc"
      - "--path.rootfs=/rootfs"
      - "--path.sysfs=/host/sys"
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)"
    deploy:
      resources:
        limits:
          memory: 64M
          cpus: "0.2"
    restart: unless-stopped

  # Docker Events Exporter
  docker-exporter:
    image: prometheusnet/docker_exporter:latest
    container_name: docker-exporter
    ports:
      - "9417:9417"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    deploy:
      resources:
        limits:
          memory: 32M
          cpus: "0.1"
    restart: unless-stopped

  # Ollama Metrics Exporter
  ollama-metrics-exporter:
    build:
      context: .
      dockerfile_inline: |
        FROM python:3.11-slim
        RUN pip install prometheus_client requests
        COPY scripts/ollama-metrics-exporter.py /app/exporter.py
        WORKDIR /app
        CMD ["python", "exporter.py"]
    container_name: ollama-metrics-exporter
    profiles: ["ai", "always"]
    ports:
      - "9435:9435"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_METRICS_PORT=9435
      - OLLAMA_SCRAPE_INTERVAL=30
    deploy:
      resources:
        limits:
          memory: 64M
          cpus: "0.2"
        reservations:
          memory: 32M
          cpus: "0.1"
    restart: unless-stopped
    depends_on:
      - ollama
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9435/metrics"]
      interval: 30s
      timeout: 10s
      retries: 3

networks:
  default:
    name: n8n-local
    driver: bridge

